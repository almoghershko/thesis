{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f163263e",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66b519ea-69ed-4618-8b3c-79e01c73053c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.9/site-packages (1.22.9)\n",
      "Requirement already satisfied: progressbar2 in /opt/conda/lib/python3.9/site-packages (4.0.0)\n",
      "Requirement already satisfied: sfdmap in /opt/conda/lib/python3.9/site-packages (0.1.1)\n",
      "Requirement already satisfied: GPUtil in /opt/conda/lib/python3.9/site-packages (1.4.0)\n",
      "Requirement already satisfied: astropy in /opt/conda/lib/python3.9/site-packages (5.0.4)\n",
      "Requirement already satisfied: botocore<1.26.0,>=1.25.9 in /opt/conda/lib/python3.9/site-packages (from boto3) (1.25.9)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.9/site-packages (from boto3) (1.0.0)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.9/site-packages (from boto3) (0.5.2)\n",
      "Requirement already satisfied: python-utils>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from progressbar2) (3.1.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from sfdmap) (1.21.5)\n",
      "Requirement already satisfied: packaging>=19.0 in /opt/conda/lib/python3.9/site-packages (from astropy) (21.3)\n",
      "Requirement already satisfied: pyerfa>=2.0 in /opt/conda/lib/python3.9/site-packages (from astropy) (2.0.0.1)\n",
      "Requirement already satisfied: PyYAML>=3.13 in /opt/conda/lib/python3.9/site-packages (from astropy) (6.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.9/site-packages (from botocore<1.26.0,>=1.25.9->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.9/site-packages (from botocore<1.26.0,>=1.25.9->boto3) (1.26.8)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=19.0->astropy) (3.0.7)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.26.0,>=1.25.9->boto3) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3 progressbar2 sfdmap GPUtil astropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe697e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import boto3\n",
    "\n",
    "# random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# local files paths\n",
    "local_home_dir_path = os.path.expanduser(\"~\")\n",
    "local_work_dir_path = os.path.join(local_home_dir_path, 'git')\n",
    "local_code_dir_path = os.path.join(local_work_dir_path , 'code')\n",
    "\n",
    "# S3 file paths\n",
    "endpoint_url = 'https://s3-west.nrp-nautilus.io'\n",
    "bucket_name = 'tau-astro'\n",
    "prefix = 'almogh'\n",
    "s3_work_dir_path = os.path.join(prefix, 'workdir3')\n",
    "s3_saves_dir_path = os.path.join(s3_work_dir_path , 'model_saves')\n",
    "s3_data_dir_path = os.path.join(s3_work_dir_path , 'data')\n",
    "s3_v2_data_ver_dir_path = os.path.join(s3_data_dir_path,'100K_V2')\n",
    "s3_v4_data_ver_dir_path = os.path.join(s3_data_dir_path,'100K_V4')\n",
    "\n",
    "s3_client = boto3.client(\"s3\", endpoint_url=endpoint_url)\n",
    "\n",
    "# adding code folder to path\n",
    "sys.path.insert(1, local_code_dir_path)\n",
    "from s3 import to_s3_npy, to_s3_pkl, from_s3_npy, from_s3_pkl, to_s3_fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17faf82a",
   "metadata": {},
   "source": [
    "# Train RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d61219c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_RF = True\n",
    "save_RF_name = '100K_V4_full_data_set'\n",
    "save_RF_dis_mat = True\n",
    "\n",
    "load_RF = False\n",
    "load_RF_name = 'simple___2021_11_27___22_09_00___standard_RF_max_depth_10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6df65602",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not (save_RF and load_RF), '\"save\" and \"load\" cant both be \"True\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddcb473",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51dc9b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data and creating dataset\n",
      "loading from uri: s3://tau-astro/almogh/workdir3/data/100K_V4/gs_100K_V4.pkl\n",
      "loading from uri: s3://tau-astro/almogh/workdir3/data/100K_V2/spec.npy\n",
      "loading from uri: s3://tau-astro/almogh/workdir3/data/wl_grid.npy\n",
      "loading from uri: s3://tau-astro/almogh/workdir3/data/100K_V4/wl_100K_V4.npy\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "print('Loading data and creating dataset')\n",
    "gs = from_s3_pkl(s3_client, bucket_name, os.path.join(s3_v4_data_ver_dir_path,'gs_100K_V4.pkl'))\n",
    "X = from_s3_npy(s3_client, bucket_name, os.path.join(s3_v2_data_ver_dir_path, 'spec.npy'))\n",
    "full_wl_grid = from_s3_npy(s3_client, bucket_name, os.path.join(s3_data_dir_path, 'wl_grid.npy'))\n",
    "wl_grid = from_s3_npy(s3_client, bucket_name, os.path.join(s3_v4_data_ver_dir_path, 'wl_100K_V4.npy'))\n",
    "start_i = (np.abs(full_wl_grid - wl_grid[0])).argmin()\n",
    "end_i = 1+(np.abs(full_wl_grid - wl_grid[-1])).argmin()\n",
    "X = X[gs.index, start_i:end_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "824c2646-6b12-4439-86a6-d92f7e86a555",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not np.any(np.isnan(X)), 'NaN!'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc57c35",
   "metadata": {},
   "source": [
    "## Creating train and test sets for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7e35cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating synthetic data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (8400 of 8400) |####################| Elapsed Time: 0:00:18 Time:  0:00:18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging\n"
     ]
    }
   ],
   "source": [
    "# creaet synthetic samples\n",
    "print('Creating synthetic data')\n",
    "shifts = False\n",
    "if shifts:\n",
    "    from CustomRandomForest import return_synthetic_data_shift, fix_nan_shifts\n",
    "    X_syn = return_synthetic_data_shift(X, 10, 3, seed)\n",
    "    X_syn = fix_nan_shifts(X_syn,10)\n",
    "else:\n",
    "    from CustomRandomForest import return_synthetic_data\n",
    "    X_syn = return_synthetic_data(X, seed)\n",
    "\n",
    "# merge the data\n",
    "print('Merging')\n",
    "from uRF_SDSS import merge_work_and_synthetic_samples\n",
    "Z, y = merge_work_and_synthetic_samples(X, X_syn)\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "Z_train, Z_test, y_train, y_test, I_train, I_test = train_test_split(Z, y, np.arange(len(y)), train_size=190000, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666ab658",
   "metadata": {},
   "source": [
    "## Fit a random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d50215",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "if load_RF:\n",
    "    \n",
    "    s3_load_dir_path = os.path.join(s3_saves_dir_path, 'RF', load_RF_name)\n",
    "    print('loading from folder (S3): {0}'.format(s3_load_dir_path))\n",
    "    \n",
    "    #I_slice = from_s3_npy(s3_client, bucket_name, os.path.join(s3_load_dir_path, 'I_slice.npy'))\n",
    "    I_train = from_s3_npy(s3_client, bucket_name, os.path.join(s3_load_dir_path, 'I_train.npy'))\n",
    "    wl_grid = from_s3_npy(s3_client, bucket_name, os.path.join(s3_load_dir_path, 'wl_grid.npy'))\n",
    "    \n",
    "    from CustomRandomForest import CustomRandomForest\n",
    "    rf = CustomRandomForest.load_s3(s3_client, bucket_name, os.path.join(s3_load_dir_path, 'crf.pkl'))\n",
    "    \n",
    "else:\n",
    "\n",
    "    # RF parameters\n",
    "    N_trees = 500\n",
    "    min_span = len(wl_grid)\n",
    "    max_span = len(wl_grid)\n",
    "    min_samples_split = 10000\n",
    "    max_features = 'sqrt'\n",
    "    max_samples = 1.0\n",
    "    max_depth = 10\n",
    "    N_snr_bins = 1\n",
    "\n",
    "    # create a random forest\n",
    "    from CustomRandomForest import CustomRandomForest\n",
    "    rf = CustomRandomForest(N_trees=N_trees,\n",
    "                            min_span=min_span,\n",
    "                            max_span=max_span,\n",
    "                            min_samples_split=min_samples_split,\n",
    "                            max_features=max_features,\n",
    "                            max_samples=max_samples,\n",
    "                            max_depth=max_depth\n",
    "                           )\n",
    "\n",
    "    # fit the forest to the data\n",
    "    rf.fit(Z_train, y_train)\n",
    "\n",
    "    if save_RF:\n",
    "\n",
    "        # create a save dir\n",
    "        from datetime import datetime\n",
    "        s3_save_dir_path = os.path.join(s3_saves_dir_path, 'RF', 'simple___' + datetime.now().strftime(\"%Y_%m_%d___%H_%M_%S\") + '___' + save_RF_name)\n",
    "        print('save folder (S3): ' + s3_save_dir_path)\n",
    "\n",
    "        # save some data\n",
    "        print('Saving numpy arrays')\n",
    "        #to_s3_npy(I_slice, s3_client, bucket_name, os.path.join(s3_save_dir_path, 'I_slice.npy'))\n",
    "        to_s3_npy(I_train, s3_client, bucket_name, os.path.join(s3_save_dir_path, 'I_train.npy'))\n",
    "        #to_s3_npy(wl_grid, s3_client, bucket_name, os.path.join(s3_save_dir_path, 'wl_grid.npy'))\n",
    "\n",
    "        # save the random forest\n",
    "        print('Saving the random forest')\n",
    "        rf.save_s3(s3_client, bucket_name, os.path.join(s3_save_dir_path, 'crf.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d517f6",
   "metadata": {},
   "source": [
    "## Evaluate the RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6c3bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_RF:\n",
    "\n",
    "    print('Predict on training set')\n",
    "    y_hat_train = rf.predict(Z_train)\n",
    "\n",
    "    print('Predict on test set')\n",
    "    y_hat_test = rf.predict(Z_test)\n",
    "\n",
    "    print('Evaluating')\n",
    "    from sklearn.metrics import classification_report\n",
    "    train_set_report = classification_report(y_train, y_hat_train)\n",
    "    test_set_report = classification_report(y_test, y_hat_test)\n",
    "    print('TRAININ-SET:')\n",
    "    print(train_set_report )\n",
    "    print('TEST-SET:')\n",
    "    print(test_set_report)\n",
    "\n",
    "    if save_RF:\n",
    "\n",
    "        from s3 import log_s3\n",
    "        log_s3(s3_client, bucket_name, path_in_bucket=s3_save_dir_path, log_name='RF_log.txt',\n",
    "            #data = '\\n'.join([original_wl_str, new_wl_str, samples_str]),\n",
    "            N_RF_train_real = sum(y_train==1),\n",
    "            N_RF_train_syn = len(y_train)-sum(y_train==1),\n",
    "            N_RF_test_real = sum(y_test==1),\n",
    "            N_RF_test_syn = len(y_test)-sum(y_test==1),\n",
    "            N_trees = rf.N_trees,\n",
    "            min_span = rf.min_span,\n",
    "            max_span = rf.max_span,\n",
    "            min_samples_split = rf.min_samples_split,\n",
    "            max_features = rf.max_features,\n",
    "            max_samples = rf.max_samples,\n",
    "            max_depth = rf.max_depth,\n",
    "            train_set_report = '\\n'+train_set_report,\n",
    "            test_set_report = '\\n'+test_set_report\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c372c75",
   "metadata": {},
   "source": [
    "###  Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d783ade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_RF:\n",
    "\n",
    "    p_test = rf.predict_proba(Z_test)\n",
    "    p_train = rf.predict_proba(Z_train)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.hist(p_train[y_train==1,1], density=True, bins=20, alpha=0.5, label='train - real')\n",
    "    plt.hist(p_train[y_train==2,2], density=True, bins=20, alpha=0.5, label='train - synthetic')\n",
    "    plt.hist(p_test[y_test==1,1], density=True, bins=20, alpha=0.5, label='test - real')\n",
    "    plt.hist(p_test[y_test==2,2], density=True, bins=20, alpha=0.5, label='test - synthetic')\n",
    "    plt.legend()\n",
    "    plt.title(\"probability distribution soft predictions\")\n",
    "    plt.ylabel(\"Pr\")\n",
    "    plt.xlabel(\"predicted probability\")\n",
    "\n",
    "    if save_RF:\n",
    "        to_s3_fig(fig, s3_client, bucket_name, os.path.join(s3_save_dir_path, 'prob_dist.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c1f3ae",
   "metadata": {},
   "source": [
    "## Calculate similarity matrix, weirdness scores and T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550444f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_RF:\n",
    "    \n",
    "    print('loading...')\n",
    "    X = from_s3_npy(s3_client, bucket_name, os.path.join(s3_load_dir_path, 'X.npy'))\n",
    "    X_leaves = from_s3_npy(s3_client, bucket_name, os.path.join(s3_load_dir_path, 'X_leaves.npy'))\n",
    "    Y_hat = from_s3_npy(s3_client, bucket_name, os.path.join(s3_load_dir_path, 'Y_hat.npy'))\n",
    "    sim_mat = from_s3_npy(s3_client, bucket_name, os.path.join(s3_load_dir_path, 'sim_mat.npy'))\n",
    "    \n",
    "else:\n",
    "\n",
    "    # Throwing the RF's test set, and the train synthetic spectra\n",
    "    \"\"\"\n",
    "    because merge_work_and_synthetic_samples concatenates the N synthetic spectra after the N real spectra,\n",
    "    all the train indices up to N are real.\n",
    "    \"\"\"\n",
    "    X = X[I_train[I_train<X.shape[0]]]\n",
    "\n",
    "    print('Applying the RF on the full dataset (real spectra only)')\n",
    "    X_leaves = rf.apply(X)\n",
    "\n",
    "    print('Predicting fully')\n",
    "    Y_hat = rf.predict_full_from_leaves(X_leaves)\n",
    "\n",
    "    print('Calculating the similarity matrix')\n",
    "    from CustomRandomForest import build_similarity_matrix\n",
    "    sim_mat = build_similarity_matrix(X_leaves, Y_hat)\n",
    "\n",
    "    if save_RF:\n",
    "\n",
    "        print('Saving the data')\n",
    "        to_s3_npy(X, s3_client, bucket_name, os.path.join(s3_save_dir_path, 'X.npy'))\n",
    "        to_s3_npy(X_leaves, s3_client, bucket_name, os.path.join(s3_save_dir_path, 'X_leaves.npy'))\n",
    "        to_s3_npy(Y_hat, s3_client, bucket_name, os.path.join(s3_save_dir_path, 'Y_hat.npy'))\n",
    "\n",
    "        print('Saving the similarity matrix')\n",
    "        to_s3_npy(sim_mat, s3_client, bucket_name, os.path.join(s3_save_dir_path, 'sim_mat.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c107aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Calculating the weirdness scores')\n",
    "dis_mat = 1 - sim_mat\n",
    "weird_scores = np.mean(dis_mat, axis=1)\n",
    "\n",
    "if save_RF:\n",
    "\n",
    "    print('Saving the weirdness scores')\n",
    "    to_s3_npy(weird_scores, s3_client, bucket_name, os.path.join(s3_save_dir_path, 'weird_scores.npy'))\n",
    "\n",
    "    if save_RF_dis_mat:\n",
    "        print('Saving the dissimilarity matrix')\n",
    "        to_s3_npy(dis_mat, s3_client, bucket_name, os.path.join(s3_save_dir_path, 'dis_mat.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9f596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_RF:\n",
    "    \n",
    "    print('loading...')\n",
    "    sne = from_s3_npy(s3_client, bucket_name, os.path.join(s3_load_dir_path, 'tsne.npy'))\n",
    "    \n",
    "else:\n",
    "\n",
    "    print('Running T-SNE')\n",
    "    from sklearn.manifold import TSNE\n",
    "    sne = TSNE(n_components=2, perplexity=25, metric='precomputed', verbose=1, random_state=seed).fit_transform(dis_mat)\n",
    "\n",
    "    if save_RF:\n",
    "\n",
    "        print('Saving T-SNE')\n",
    "        to_s3_npy(sne, s3_client, bucket_name, os.path.join(s3_save_dir_path, 'tsne.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db9ca51",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772ac628",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_RF:\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    tmp = plt.hist(weird_scores, bins=60, color=\"g\")\n",
    "    plt.title(\"Weirdness score histogram\")\n",
    "    plt.ylabel(\"N\")\n",
    "    plt.xlabel(\"weirdness score\")\n",
    "\n",
    "    if save_RF:\n",
    "        to_s3_fig(fig, s3_client, bucket_name, os.path.join(s3_save_dir_path, 'weirdness_scores_histogram.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c575e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_RF:\n",
    "\n",
    "    distances = dis_mat[np.tril_indices(dis_mat.shape[0], -1)]\n",
    "\n",
    "    fig = plt.figure()\n",
    "    tmp = plt.hist(distances, bins=100)\n",
    "    plt.title(\"Distances histogram\")\n",
    "    plt.ylabel(\"N\")\n",
    "    plt.xlabel(\"distance\")\n",
    "\n",
    "    if save_RF:\n",
    "        to_s3_fig(fig, s3_client, bucket_name, os.path.join(s3_save_dir_path, 'distances_histogram.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e13df62",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_RF:\n",
    "    \n",
    "    sne_f1 = sne[:, 0]\n",
    "    sne_f2 = sne[:, 1]\n",
    "\n",
    "    fig = plt.figure(figsize=(10,7))\n",
    "    ax = fig.add_subplot(111)\n",
    "    im_scat = ax.scatter(sne_f1, sne_f2, s=3, c=weird_scores, cmap=plt.cm.get_cmap('jet'), picker=1)\n",
    "    ax.set_xlabel('t-SNE Feature 1')\n",
    "    ax.set_ylabel('t-SNE Feature 2')\n",
    "    ax.set_title(r't-SNE Scatter Plot Colored by Weirdness score')\n",
    "    clb = fig.colorbar(im_scat, ax=ax)\n",
    "    clb.ax.set_ylabel('Weirdness', rotation=270)\n",
    "    plt.show()\n",
    "\n",
    "    if save_RF:\n",
    "        to_s3_fig(fig, s3_client, bucket_name, os.path.join(s3_save_dir_path, 'tsne_colored_by_weirdness.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0215a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "I_real_train = I_train[I_train<10000]\n",
    "snr = gs.snMedian.iloc[I_real_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17e62ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_RF:\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,7))\n",
    "    ax = fig.add_subplot(111)\n",
    "    import matplotlib.colors as colors\n",
    "    im_scat = ax.scatter(sne_f1, sne_f2, s=3, c=snr, cmap=plt.cm.get_cmap('jet'), norm=colors.LogNorm(vmin=snr.min(), vmax=80))\n",
    "    ax.set_xlabel('t-SNE Feature 1')\n",
    "    ax.set_ylabel('t-SNE Feature 2')\n",
    "    ax.set_title(r't-SNE Scatter Plot Colored by SNR')\n",
    "    clb = fig.colorbar(im_scat, ax=ax)\n",
    "    clb.ax.set_ylabel('SNR', rotation=270)\n",
    "    plt.show()\n",
    "\n",
    "if save_RF:\n",
    "    to_s3_fig(fig, s3_client, bucket_name, os.path.join(s3_save_dir_path, 'tsne_colored_by_snr.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0d4494-1ae4-4810-b44d-00d07b971687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
