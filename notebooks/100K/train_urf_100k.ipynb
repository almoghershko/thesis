{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f163263e",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30ed1477-fc03-4bff-8be7-c95ae68c4201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.9/site-packages (1.21.27)\n",
      "Requirement already satisfied: botocore<1.25.0,>=1.24.27 in /opt/conda/lib/python3.9/site-packages (from boto3) (1.24.27)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.9/site-packages (from boto3) (0.5.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.9/site-packages (from boto3) (1.0.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.9/site-packages (from botocore<1.25.0,>=1.24.27->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.9/site-packages (from botocore<1.25.0,>=1.24.27->boto3) (1.26.8)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.25.0,>=1.24.27->boto3) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "888522ec-a0a8-4b6b-a2bb-84820cdb3e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: progressbar2 in /opt/conda/lib/python3.9/site-packages (4.0.0)\n",
      "Requirement already satisfied: python-utils>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from progressbar2) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install progressbar2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6796912-dbf5-4967-a19d-18fea50e3543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: astropy in /opt/conda/lib/python3.9/site-packages (5.0.3)\n",
      "Requirement already satisfied: packaging>=19.0 in /opt/conda/lib/python3.9/site-packages (from astropy) (21.3)\n",
      "Requirement already satisfied: PyYAML>=3.13 in /opt/conda/lib/python3.9/site-packages (from astropy) (6.0)\n",
      "Requirement already satisfied: numpy>=1.18 in /opt/conda/lib/python3.9/site-packages (from astropy) (1.21.5)\n",
      "Requirement already satisfied: pyerfa>=2.0 in /opt/conda/lib/python3.9/site-packages (from astropy) (2.0.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=19.0->astropy) (3.0.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install astropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c693ca61-71e6-42ed-96c6-efaae12ec3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sfdmap in /opt/conda/lib/python3.9/site-packages (0.1.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from sfdmap) (1.21.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install sfdmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe697e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import boto3\n",
    "\n",
    "# random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# local files paths\n",
    "local_home_dir_path = os.path.expanduser(\"~\")\n",
    "local_work_dir_path = os.path.join(local_home_dir_path, 'git')\n",
    "local_code_dir_path = os.path.join(local_work_dir_path , 'code')\n",
    "\n",
    "# S3 file paths\n",
    "endpoint_url = 'https://s3.nautilus.optiputer.net'\n",
    "bucket_name = 'tau-astro'\n",
    "prefix = 'almogh'\n",
    "s3_work_dir_path = os.path.join(prefix, 'workdir3')\n",
    "s3_saves_dir_path = os.path.join(s3_work_dir_path , 'model_saves')\n",
    "s3_data_dir_path = os.path.join(s3_work_dir_path , 'data')\n",
    "s3_data_ver_dir_path = os.path.join(s3_data_dir_path,'100K_V1')\n",
    "\n",
    "s3_client = boto3.client(\"s3\", endpoint_url=endpoint_url)\n",
    "\n",
    "# adding code folder to path\n",
    "sys.path.insert(1, local_code_dir_path)\n",
    "from s3 import to_s3_npy, to_s3_pkl, from_s3_npy, from_s3_pkl, to_s3_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6726e99-9c3b-4c2e-8dd5-c91fd09704fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save folder (S3): almogh/workdir3/model_saves/RF/large_URF_100K_dataset__2022_03_27___14_08_15\n"
     ]
    }
   ],
   "source": [
    "retrain = True\n",
    "save_RF_dis_mat = False\n",
    "save_RF_name = 'large_URF_100K_dataset'\n",
    "\n",
    "if retrain:\n",
    "    # create a save dir\n",
    "    from datetime import datetime\n",
    "    save_RF_dir = save_RF_name + '__' + datetime.now().strftime(\"%Y_%m_%d___%H_%M_%S\")\n",
    "else:\n",
    "    save_RF_dir = 'large_URF_100K_dataset2022_03_26___22_54_46/'\n",
    "s3_urf_save_dir_path = os.path.join(s3_saves_dir_path, 'RF', save_RF_dir)\n",
    "print('save folder (S3): ' + s3_urf_save_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17faf82a",
   "metadata": {},
   "source": [
    "# Train RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddcb473",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98b02d4c-832a-4a81-bcc4-4460ac97788f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data and creating dataset\n",
      "loading from uri: s3://tau-astro/almogh/workdir3/data/100K_V1/spec.npy\n",
      "loading from uri: s3://tau-astro/almogh/workdir3/data/100K_V1/wl_grid.npy\n",
      "wavelength grid: 3825.0-7725.0 [A], length=7800.\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "print('Loading data and creating dataset')\n",
    "X = from_s3_npy(s3_client, bucket_name, os.path.join(s3_data_ver_dir_path, 'spec.npy'))\n",
    "wl_grid = from_s3_npy(s3_client, bucket_name, os.path.join(s3_data_ver_dir_path, 'wl_grid.npy'))\n",
    "print('wavelength grid: {0}-{1} [A], length={2}.'.format(min(wl_grid),max(wl_grid),len(wl_grid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c45d554b-69fa-427e-a718-d744ab9b0b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from uri: s3://tau-astro/almogh/workdir3/data/100K_V1/gs.pkl\n",
      "loading from uri: s3://tau-astro/almogh/workdir3/data/100K_V1/gs_train.pkl\n",
      "loading from uri: s3://tau-astro/almogh/workdir3/data/100K_V1/gs_test.pkl\n"
     ]
    }
   ],
   "source": [
    "gs = from_s3_pkl(s3_client, bucket_name, os.path.join(s3_data_ver_dir_path,'gs.pkl'))\n",
    "gs_train = from_s3_pkl(s3_client, bucket_name, os.path.join(s3_data_ver_dir_path,'gs_train.pkl'))\n",
    "gs_test = from_s3_pkl(s3_client, bucket_name, os.path.join(s3_data_ver_dir_path,'gs_test.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1f15ff-1663-4d84-9e99-dccbad4768d6",
   "metadata": {},
   "source": [
    "## Create a test set for the URF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce8a083a-8a6a-4455-969f-e6514972b712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving train/test indices\n",
      "saving to uri: s3://tau-astro/almogh/workdir3/model_saves/RF/large_URF_100K_dataset__2022_03_27___14_08_15/I_train.npy\n",
      "saving to uri: s3://tau-astro/almogh/workdir3/model_saves/RF/large_URF_100K_dataset__2022_03_27___14_08_15/I_test.npy\n"
     ]
    }
   ],
   "source": [
    "if retrain:\n",
    "    # Need to create a 5K test set for the URF, which doesn't include the train and the test set we already split\n",
    "    gs_remain = gs[~gs['specobjid'].isin(gs_train['specobjid'])]\n",
    "    gs_remain = gs_remain[~gs_remain['specobjid'].isin(gs_test['specobjid'])]\n",
    "    gs_URF_test = gs_remain.sample(5000, random_state=seed)\n",
    "    I_test = gs_URF_test.index.to_list()\n",
    "    I_train = gs.index.difference(gs_URF_test.index).to_list()\n",
    "    \n",
    "    print('Saving train/test indices')\n",
    "    to_s3_npy(I_train, s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'I_train.npy'))\n",
    "    to_s3_npy(I_test, s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'I_test.npy'))\n",
    "else:\n",
    "    print('Loading train/test indices')\n",
    "    I_train = from_s3_npy(s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'I_train.npy'))\n",
    "    I_test = from_s3_npy(s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'I_test.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8635fe5a-c7c4-4549-980d-f54eb7ff120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[I_train]\n",
    "X_test = X[I_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc57c35",
   "metadata": {},
   "source": [
    "## Creating train and test sets for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7e35cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating synthetic data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (7800 of 7800) |####################| Elapsed Time: 0:00:44 Time:  0:00:44\n",
      "100% (7800 of 7800) |####################| Elapsed Time: 0:00:01 Time:  0:00:01\n"
     ]
    }
   ],
   "source": [
    "if retrain:\n",
    "    # creaet synthetic samples\n",
    "    print('Creating synthetic data')\n",
    "    from CustomRandomForest import return_synthetic_data\n",
    "    from uRF_SDSS import merge_work_and_synthetic_samples\n",
    "\n",
    "    X_train_syn = return_synthetic_data(X_train, seed)\n",
    "    X_test_syn = return_synthetic_data(X_test, seed)\n",
    "\n",
    "    Z_train, y_train = merge_work_and_synthetic_samples(X_train, X_train_syn)\n",
    "    Z_test, y_test = merge_work_and_synthetic_samples(X_test, X_test_syn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666ab658",
   "metadata": {},
   "source": [
    "## Fit a random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d50215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting RF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "from CustomRandomForest import CustomRandomForest\n",
    "if retrain:\n",
    "    # RF parameters\n",
    "    N_trees = 500\n",
    "    min_span = len(wl_grid)\n",
    "    max_span = len(wl_grid)\n",
    "    min_samples_split = 10000\n",
    "    max_features = 'sqrt'\n",
    "    max_samples = 1.0\n",
    "    max_depth = 10\n",
    "    N_snr_bins = 1\n",
    "\n",
    "    # create a random forest\n",
    "    rf = CustomRandomForest(N_trees=N_trees,\n",
    "                            min_span=min_span,\n",
    "                            max_span=max_span,\n",
    "                            min_samples_split=min_samples_split,\n",
    "                            max_features=max_features,\n",
    "                            max_samples=max_samples,\n",
    "                            max_depth=max_depth\n",
    "                           )\n",
    "\n",
    "    # fit the forest to the data\n",
    "    print('fitting RF...')\n",
    "    rf.fit(Z_train, y_train, prefer=\"processes\")\n",
    "    print('done.')\n",
    "    \n",
    "else:\n",
    "    print('Loading RF...')\n",
    "    rf = CustomRandomForest.load_s3(s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'crf.pkl'))\n",
    "    print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5808412c-e666-4886-af33-dfc139163be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if retrain:\n",
    "    # save the random forest\n",
    "    print('Saving the random forest')\n",
    "    rf.save_s3(s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'crf.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d517f6",
   "metadata": {},
   "source": [
    "## Evaluate the RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6c3bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if retrain:\n",
    "    print('Predict on training set')\n",
    "    y_hat_train = rf.predict(Z_train)\n",
    "\n",
    "    print('Predict on test set')\n",
    "    y_hat_test = rf.predict(Z_test)\n",
    "\n",
    "    print('Evaluating')\n",
    "    from sklearn.metrics import classification_report\n",
    "    train_set_report = classification_report(y_train, y_hat_train)\n",
    "    test_set_report = classification_report(y_test, y_hat_test)\n",
    "    print('TRAININ-SET:')\n",
    "    print(train_set_report )\n",
    "    print('TEST-SET:')\n",
    "    print(test_set_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d686b84-3ece-4288-b4bd-9ca55c812fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if retrain:\n",
    "    from s3 import log_s3\n",
    "    log_s3(s3_client, bucket_name, path_in_bucket=s3_urf_save_dir_path, log_name='RF_log.txt',\n",
    "        N_RF_train_real = sum(y_train==1),\n",
    "        N_RF_train_syn = len(y_train)-sum(y_train==1),\n",
    "        N_RF_test_real = sum(y_test==1),\n",
    "        N_RF_test_syn = len(y_test)-sum(y_test==1),\n",
    "        N_trees = rf.N_trees,\n",
    "        min_span = rf.min_span,\n",
    "        max_span = rf.max_span,\n",
    "        min_samples_split = rf.min_samples_split,\n",
    "        max_features = rf.max_features,\n",
    "        max_samples = rf.max_samples,\n",
    "        max_depth = rf.max_depth,\n",
    "        train_set_report = '\\n'+train_set_report,\n",
    "        test_set_report = '\\n'+test_set_report\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c372c75",
   "metadata": {},
   "source": [
    "###  Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d783ade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if retrain:\n",
    "    p_test = rf.predict_proba(Z_test)\n",
    "    p_train = rf.predict_proba(Z_train)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.hist(p_train[y_train==1,1], density=True, bins=20, alpha=0.5, label='train - real')\n",
    "    plt.hist(p_train[y_train==2,2], density=True, bins=20, alpha=0.5, label='train - synthetic')\n",
    "    plt.hist(p_test[y_test==1,1], density=True, bins=20, alpha=0.5, label='test - real')\n",
    "    plt.hist(p_test[y_test==2,2], density=True, bins=20, alpha=0.5, label='test - synthetic')\n",
    "    plt.legend()\n",
    "    plt.title(\"probability distribution soft predictions\")\n",
    "    plt.ylabel(\"Pr\")\n",
    "    plt.xlabel(\"predicted probability\")\n",
    "\n",
    "    to_s3_fig(fig, s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'prob_dist.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c1f3ae",
   "metadata": {},
   "source": [
    "## Calculate similarity matrix, weirdness scores and T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e22e94-5369-4f12-a8ac-2e5714158708",
   "metadata": {},
   "outputs": [],
   "source": [
    "if retrain:\n",
    "    print('Applying the RF on the full dataset (real spectra only)')\n",
    "    X_train_leaves = rf.apply(X_train).astype(np.float32)\n",
    "\n",
    "    print('Predicting fully')\n",
    "    Y_train_hat = rf.predict_full_from_leaves(X_train_leaves).astype(np.float32)\n",
    "    \n",
    "    print('Saving the data')\n",
    "    to_s3_npy(X_train_leaves, s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'X_train_leaves.npy'))\n",
    "    to_s3_npy(Y_train_hat, s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'Y_train_hat.npy'))\n",
    "else:\n",
    "    print('loading the data')\n",
    "    X_train_leaves = from_s3_npy(s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'X_train_leaves.npy'))\n",
    "    Y_train_hat = from_s3_npy(s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'Y_train_hat.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550444f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if retrain:\n",
    "    print('Calculating the distance matrix')\n",
    "    from CustomRandomForest import build_distance_matrix\n",
    "    dist_mat = build_distance_matrix(X_train_leaves, Y_train_hat)\n",
    "\n",
    "    if save_RF_dis_mat:\n",
    "        print('Saving the distance matrix')\n",
    "        to_s3_npy(dist_mat, s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'dist_mat.npy'))\n",
    "else:\n",
    "    if save_RF_dis_mat:\n",
    "        print('Loading the distance matrix')\n",
    "        dist_mat = load_s3_npy(s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'dist_mat.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c107aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if retrain:\n",
    "    print('Calculating the weirdness scores')\n",
    "    weird_scores = np.mean(dist_mat, axis=1)\n",
    "\n",
    "    print('Saving the weirdness scores')\n",
    "    to_s3_npy(weird_scores, s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'weird_scores.npy'))\n",
    "else:\n",
    "    print('Loading the weirdness scores')\n",
    "    weird_scores = from_s3_npy(s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'weird_scores.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9f596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if retrain:\n",
    "    print('Running T-SNE')\n",
    "    from sklearn.manifold import TSNE\n",
    "    sne = TSNE(n_components=2, perplexity=25, metric='precomputed', verbose=1, random_state=seed).fit_transform(dist_mat)\n",
    "\n",
    "    print('Saving T-SNE')\n",
    "    to_s3_npy(sne, s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'tsne.npy'))\n",
    "else:\n",
    "    print('Loading T-SNE')\n",
    "    sne = to_s3_npy(s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'tsne.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db9ca51",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772ac628",
   "metadata": {},
   "outputs": [],
   "source": [
    "if retrain:\n",
    "    fig = plt.figure()\n",
    "    tmp = plt.hist(weird_scores, bins=60, color=\"g\")\n",
    "    plt.title(\"Weirdness score histogram\")\n",
    "    plt.ylabel(\"N\")\n",
    "    plt.xlabel(\"weirdness score\")\n",
    "\n",
    "    to_s3_fig(fig, s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'weirdness_scores_histogram.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c575e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "if retrain:\n",
    "\n",
    "    fig = plt.figure()\n",
    "    tmp = plt.hist(dist_mat[np.tril_indices(dist_mat.shape[0], -1)], bins=100)\n",
    "    plt.title(\"Distances histogram\")\n",
    "    plt.ylabel(\"N\")\n",
    "    plt.xlabel(\"distance\")\n",
    "\n",
    "    to_s3_fig(fig, s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'distances_histogram.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e13df62",
   "metadata": {},
   "outputs": [],
   "source": [
    "if retrain:\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,7))\n",
    "    ax = fig.add_subplot(111)\n",
    "    im_scat = ax.scatter(sne[:, 0], sne[:, 1], s=3, c=weird_scores, cmap=plt.cm.get_cmap('jet'), picker=1)\n",
    "    ax.set_xlabel('t-SNE Feature 1')\n",
    "    ax.set_ylabel('t-SNE Feature 2')\n",
    "    ax.set_title(r't-SNE Scatter Plot Colored by Weirdness score')\n",
    "    clb = fig.colorbar(im_scat, ax=ax)\n",
    "    clb.ax.set_ylabel('Weirdness', rotation=270)\n",
    "    plt.show()\n",
    "\n",
    "    to_s3_fig(fig, s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'tsne_colored_by_weirdness.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17e62ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111)\n",
    "import matplotlib.colors as colors\n",
    "snr = gs_train.loc[I_train].snMedian\n",
    "im_scat = ax.scatter(sne[:,0], sne[:,1], s=3, c=snr, cmap=plt.cm.get_cmap('jet'), norm=colors.LogNorm(vmin=snr.min(), vmax=80))\n",
    "ax.set_xlabel('t-SNE Feature 1')\n",
    "ax.set_ylabel('t-SNE Feature 2')\n",
    "ax.set_title(r't-SNE Scatter Plot Colored by SNR')\n",
    "clb = fig.colorbar(im_scat, ax=ax)\n",
    "clb.ax.set_ylabel('SNR', rotation=270)\n",
    "plt.show()\n",
    "\n",
    "to_s3_fig(fig, s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'tsne_colored_by_snr.png'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
