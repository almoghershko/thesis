{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f163263e",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30ed1477-fc03-4bff-8be7-c95ae68c4201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Using cached boto3-1.21.27-py3-none-any.whl (132 kB)\n",
      "Collecting s3transfer<0.6.0,>=0.5.0\n",
      "  Using cached s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Using cached jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
      "Collecting botocore<1.25.0,>=1.24.27\n",
      "  Using cached botocore-1.24.27-py3-none-any.whl (8.6 MB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.9/site-packages (from botocore<1.25.0,>=1.24.27->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.9/site-packages (from botocore<1.25.0,>=1.24.27->boto3) (1.26.8)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.25.0,>=1.24.27->boto3) (1.16.0)\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.21.27 botocore-1.24.27 jmespath-1.0.0 s3transfer-0.5.2\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "888522ec-a0a8-4b6b-a2bb-84820cdb3e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting progressbar2\n",
      "  Using cached progressbar2-4.0.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting python-utils>=3.0.0\n",
      "  Using cached python_utils-3.1.0-py2.py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-utils, progressbar2\n",
      "Successfully installed progressbar2-4.0.0 python-utils-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install progressbar2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6796912-dbf5-4967-a19d-18fea50e3543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting astropy\n",
      "  Using cached astropy-5.0.3-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (11.1 MB)\n",
      "Requirement already satisfied: packaging>=19.0 in /opt/conda/lib/python3.9/site-packages (from astropy) (21.3)\n",
      "Requirement already satisfied: numpy>=1.18 in /opt/conda/lib/python3.9/site-packages (from astropy) (1.21.5)\n",
      "Collecting pyerfa>=2.0\n",
      "  Using cached pyerfa-2.0.0.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (742 kB)\n",
      "Requirement already satisfied: PyYAML>=3.13 in /opt/conda/lib/python3.9/site-packages (from astropy) (6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=19.0->astropy) (3.0.7)\n",
      "Installing collected packages: pyerfa, astropy\n",
      "Successfully installed astropy-5.0.3 pyerfa-2.0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install astropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c693ca61-71e6-42ed-96c6-efaae12ec3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sfdmap\n",
      "  Using cached sfdmap-0.1.1-py3-none-any.whl\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from sfdmap) (1.21.5)\n",
      "Installing collected packages: sfdmap\n",
      "Successfully installed sfdmap-0.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sfdmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe697e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import boto3\n",
    "\n",
    "# random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# local files paths\n",
    "local_home_dir_path = os.path.expanduser(\"~\")\n",
    "local_work_dir_path = os.path.join(local_home_dir_path, 'git')\n",
    "local_code_dir_path = os.path.join(local_work_dir_path , 'code')\n",
    "\n",
    "# S3 file paths\n",
    "endpoint_url = 'https://s3.nautilus.optiputer.net'\n",
    "bucket_name = 'tau-astro'\n",
    "prefix = 'almogh'\n",
    "s3_work_dir_path = os.path.join(prefix, 'workdir3')\n",
    "s3_saves_dir_path = os.path.join(s3_work_dir_path , 'model_saves')\n",
    "s3_data_dir_path = os.path.join(s3_work_dir_path , 'data')\n",
    "s3_data_ver_dir_path = os.path.join(s3_data_dir_path,'100K_V1')\n",
    "\n",
    "s3_client = boto3.client(\"s3\", endpoint_url=endpoint_url)\n",
    "\n",
    "# adding code folder to path\n",
    "sys.path.insert(1, local_code_dir_path)\n",
    "from s3 import to_s3_npy, to_s3_pkl, from_s3_npy, from_s3_pkl, to_s3_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6726e99-9c3b-4c2e-8dd5-c91fd09704fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save folder (S3): almogh/workdir3/model_saves/RF/large_URF_100K_dataset2022_03_26___22_54_46/\n"
     ]
    }
   ],
   "source": [
    "retrain = True\n",
    "save_RF_dis_mat = False\n",
    "save_RF_name = 'large_URF_100K_dataset'\n",
    "\n",
    "if retrain:\n",
    "    # create a save dir\n",
    "    from datetime import datetime\n",
    "    save_RF_dir = save_RF_name + '__' + datetime.now().strftime(\"%Y_%m_%d___%H_%M_%S\")\n",
    "else:\n",
    "    save_RF_dir = 'large_URF_100K_dataset2022_03_26___22_54_46/'\n",
    "s3_urf_save_dir_path = os.path.join(s3_saves_dir_path, 'RF', save_RF_dir)\n",
    "print('save folder (S3): ' + s3_urf_save_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17faf82a",
   "metadata": {},
   "source": [
    "# Train RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddcb473",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98b02d4c-832a-4a81-bcc4-4460ac97788f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data and creating dataset\n",
      "loading from uri: s3://tau-astro/almogh/workdir3/data/100K_V1/spec.npy\n",
      "loading from uri: s3://tau-astro/almogh/workdir3/data/100K_V1/wl_grid.npy\n",
      "wavelength grid: 3825.0-7725.0 [A], length=7800.\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "print('Loading data and creating dataset')\n",
    "X = from_s3_npy(s3_client, bucket_name, os.path.join(s3_data_ver_dir_path, 'spec.npy'))\n",
    "wl_grid = from_s3_npy(s3_client, bucket_name, os.path.join(s3_data_ver_dir_path, 'wl_grid.npy'))\n",
    "print('wavelength grid: {0}-{1} [A], length={2}.'.format(min(wl_grid),max(wl_grid),len(wl_grid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c45d554b-69fa-427e-a718-d744ab9b0b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from uri: s3://tau-astro/almogh/workdir3/data/100K_V1/gs.pkl\n",
      "loading from uri: s3://tau-astro/almogh/workdir3/data/100K_V1/gs_train.pkl\n",
      "loading from uri: s3://tau-astro/almogh/workdir3/data/100K_V1/gs_test.pkl\n"
     ]
    }
   ],
   "source": [
    "gs = from_s3_pkl(s3_client, bucket_name, os.path.join(s3_data_ver_dir_path,'gs.pkl'))\n",
    "gs_train = from_s3_pkl(s3_client, bucket_name, os.path.join(s3_data_ver_dir_path,'gs_train.pkl'))\n",
    "gs_test = from_s3_pkl(s3_client, bucket_name, os.path.join(s3_data_ver_dir_path,'gs_test.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1f15ff-1663-4d84-9e99-dccbad4768d6",
   "metadata": {},
   "source": [
    "## Create a test set for the URF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce8a083a-8a6a-4455-969f-e6514972b712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train/test indices\n",
      "loading from uri: s3://tau-astro/almogh/workdir3/model_saves/RF/large_URF_100K_dataset2022_03_26___22_54_46/I_train.npy\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (404) when calling the HeadObject operation: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading train/test indices\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m     I_train \u001b[38;5;241m=\u001b[39m \u001b[43mfrom_s3_npy\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms3_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms3_urf_save_dir_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mI_train.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     I_test \u001b[38;5;241m=\u001b[39m from_s3_npy(s3_client, bucket_name, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(s3_urf_save_dir_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI_test.npy\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/git/code/s3.py:70\u001b[0m, in \u001b[0;36mfrom_s3_npy\u001b[0;34m(s3_client, bucket_name, path_in_bucket)\u001b[0m\n\u001b[1;32m     68\u001b[0m bytes_ \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO()\n\u001b[1;32m     69\u001b[0m parsed_s3 \u001b[38;5;241m=\u001b[39m urlparse(s3_uri)\n\u001b[0;32m---> 70\u001b[0m \u001b[43ms3_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_fileobj\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mFileobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbytes_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparsed_s3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparsed_s3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m bytes_\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mload(bytes_, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/boto3/s3/inject.py:795\u001b[0m, in \u001b[0;36mdownload_fileobj\u001b[0;34m(self, Bucket, Key, Fileobj, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m create_transfer_manager(\u001b[38;5;28mself\u001b[39m, config) \u001b[38;5;28;01mas\u001b[39;00m manager:\n\u001b[1;32m    788\u001b[0m     future \u001b[38;5;241m=\u001b[39m manager\u001b[38;5;241m.\u001b[39mdownload(\n\u001b[1;32m    789\u001b[0m         bucket\u001b[38;5;241m=\u001b[39mBucket,\n\u001b[1;32m    790\u001b[0m         key\u001b[38;5;241m=\u001b[39mKey,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    793\u001b[0m         subscribers\u001b[38;5;241m=\u001b[39msubscribers,\n\u001b[1;32m    794\u001b[0m     )\n\u001b[0;32m--> 795\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/s3transfer/futures.py:103\u001b[0m, in \u001b[0;36mTransferFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;66;03m# Usually the result() method blocks until the transfer is done,\u001b[39;00m\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;66;03m# however if a KeyboardInterrupt is raised we want want to exit\u001b[39;00m\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;66;03m# out of this and propagate the exception.\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_coordinator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/s3transfer/futures.py:266\u001b[0m, in \u001b[0;36mTransferCoordinator.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# Once done waiting, raise an exception if present or return the\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# final result.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/s3transfer/tasks.py:269\u001b[0m, in \u001b[0;36mSubmissionTask._main\u001b[0;34m(self, transfer_future, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_coordinator\u001b[38;5;241m.\u001b[39mset_status_to_running()\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;66;03m# Call the submit method to start submitting tasks to execute the\u001b[39;00m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;66;03m# transfer.\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_submit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransfer_future\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransfer_future\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;66;03m# If there was an exception raised during the submission of task\u001b[39;00m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;66;03m# there is a chance that the final task that signals if a transfer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m \n\u001b[1;32m    282\u001b[0m     \u001b[38;5;66;03m# Set the exception, that caused the process to fail.\u001b[39;00m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_and_set_exception(e)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/s3transfer/download.py:354\u001b[0m, in \u001b[0;36mDownloadSubmissionTask._submit\u001b[0;34m(self, client, config, osutil, request_executor, io_executor, transfer_future, bandwidth_limiter)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;124;03m:param client: The client associated with the transfer manager\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m    downloading streams\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transfer_future\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39msize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;66;03m# If a size was not provided figure out the size for the\u001b[39;00m\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;66;03m# user.\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mBucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransfer_future\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mKey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransfer_future\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtransfer_future\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextra_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    359\u001b[0m     transfer_future\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mprovide_transfer_size(\n\u001b[1;32m    360\u001b[0m         response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContentLength\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    361\u001b[0m     )\n\u001b[1;32m    363\u001b[0m download_output_manager \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_download_output_manager_cls(\n\u001b[1;32m    364\u001b[0m     transfer_future, osutil\n\u001b[1;32m    365\u001b[0m )(osutil, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_coordinator, io_executor)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/botocore/client.py:401\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m py_operation_name)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 401\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/botocore/client.py:731\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    729\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m parsed_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    730\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 731\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (404) when calling the HeadObject operation: Not Found"
     ]
    }
   ],
   "source": [
    "if retrain:\n",
    "    # Need to create a 5K test set for the URF, which doesn't include the train and the test set we already split\n",
    "    gs_remain = gs[~gs['specobjid'].isin(gs_train['specobjid'])]\n",
    "    gs_remain = gs_remain[~gs_remain['specobjid'].isin(gs_test['specobjid'])]\n",
    "    gs_URF_test = gs_remain.sample(5000, random_state=seed)\n",
    "    I_test = gs_URF_test.index.to_list()\n",
    "    I_train = gs.index.difference(gs_URF_test.index).to_list()\n",
    "    \n",
    "    print('Saving train/test indices')\n",
    "    to_s3_npy(I_train, s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'I_train.npy'))\n",
    "    to_s3_npy(I_test, s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'I_test.npy'))\n",
    "else:\n",
    "    print('Loading train/test indices')\n",
    "    I_train = from_s3_npy(s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'I_train.npy'))\n",
    "    I_test = from_s3_npy(s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'I_test.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8635fe5a-c7c4-4549-980d-f54eb7ff120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[I_train]\n",
    "X_test = X[I_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc57c35",
   "metadata": {},
   "source": [
    "## Creating train and test sets for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e35cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if retrain:\n",
    "    # creaet synthetic samples\n",
    "    print('Creating synthetic data')\n",
    "    from CustomRandomForest import return_synthetic_data\n",
    "    from uRF_SDSS import merge_work_and_synthetic_samples\n",
    "\n",
    "    X_train_syn = return_synthetic_data(X_train, seed)\n",
    "    X_test_syn = return_synthetic_data(X_test, seed)\n",
    "\n",
    "    Z_train, y_train = merge_work_and_synthetic_samples(X_train, X_train_syn)\n",
    "    Z_test, y_test = merge_work_and_synthetic_samples(X_test, X_test_syn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666ab658",
   "metadata": {},
   "source": [
    "## Fit a random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d50215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CustomRandomForest import CustomRandomForest\n",
    "if retrain:\n",
    "    # RF parameters\n",
    "    N_trees = 500\n",
    "    min_span = len(wl_grid)\n",
    "    max_span = len(wl_grid)\n",
    "    min_samples_split = 10000\n",
    "    max_features = 'sqrt'\n",
    "    max_samples = 1.0\n",
    "    max_depth = 10\n",
    "    N_snr_bins = 1\n",
    "\n",
    "    # create a random forest\n",
    "    rf = CustomRandomForest(N_trees=N_trees,\n",
    "                            min_span=min_span,\n",
    "                            max_span=max_span,\n",
    "                            min_samples_split=min_samples_split,\n",
    "                            max_features=max_features,\n",
    "                            max_samples=max_samples,\n",
    "                            max_depth=max_depth\n",
    "                           )\n",
    "\n",
    "    # fit the forest to the data\n",
    "    print('fitting RF...')\n",
    "    rf.fit(Z_train, y_train, prefer=\"processes\")\n",
    "    print('done.')\n",
    "    \n",
    "else:\n",
    "    print('Loading RF...')\n",
    "    rf = CustomRandomForest.load_s3(s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'crf.pkl'))\n",
    "    print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5808412c-e666-4886-af33-dfc139163be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if retrain:\n",
    "    # save the random forest\n",
    "    print('Saving the random forest')\n",
    "    rf.save_s3(s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'crf.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d517f6",
   "metadata": {},
   "source": [
    "## Evaluate the RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6c3bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if retrain:\n",
    "    print('Predict on training set')\n",
    "    y_hat_train = rf.predict(Z_train)\n",
    "\n",
    "    print('Predict on test set')\n",
    "    y_hat_test = rf.predict(Z_test)\n",
    "\n",
    "    print('Evaluating')\n",
    "    from sklearn.metrics import classification_report\n",
    "    train_set_report = classification_report(y_train, y_hat_train)\n",
    "    test_set_report = classification_report(y_test, y_hat_test)\n",
    "    print('TRAININ-SET:')\n",
    "    print(train_set_report )\n",
    "    print('TEST-SET:')\n",
    "    print(test_set_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d686b84-3ece-4288-b4bd-9ca55c812fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if retrain:\n",
    "    from s3 import log_s3\n",
    "    log_s3(s3_client, bucket_name, path_in_bucket=s3_urf_save_dir_path, log_name='RF_log.txt',\n",
    "        N_RF_train_real = sum(y_train==1),\n",
    "        N_RF_train_syn = len(y_train)-sum(y_train==1),\n",
    "        N_RF_test_real = sum(y_test==1),\n",
    "        N_RF_test_syn = len(y_test)-sum(y_test==1),\n",
    "        N_trees = rf.N_trees,\n",
    "        min_span = rf.min_span,\n",
    "        max_span = rf.max_span,\n",
    "        min_samples_split = rf.min_samples_split,\n",
    "        max_features = rf.max_features,\n",
    "        max_samples = rf.max_samples,\n",
    "        max_depth = rf.max_depth,\n",
    "        train_set_report = '\\n'+train_set_report,\n",
    "        test_set_report = '\\n'+test_set_report\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c372c75",
   "metadata": {},
   "source": [
    "###  Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d783ade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if retrain:\n",
    "    p_test = rf.predict_proba(Z_test)\n",
    "    p_train = rf.predict_proba(Z_train)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.hist(p_train[y_train==1,1], density=True, bins=20, alpha=0.5, label='train - real')\n",
    "    plt.hist(p_train[y_train==2,2], density=True, bins=20, alpha=0.5, label='train - synthetic')\n",
    "    plt.hist(p_test[y_test==1,1], density=True, bins=20, alpha=0.5, label='test - real')\n",
    "    plt.hist(p_test[y_test==2,2], density=True, bins=20, alpha=0.5, label='test - synthetic')\n",
    "    plt.legend()\n",
    "    plt.title(\"probability distribution soft predictions\")\n",
    "    plt.ylabel(\"Pr\")\n",
    "    plt.xlabel(\"predicted probability\")\n",
    "\n",
    "    to_s3_fig(fig, s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'prob_dist.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c1f3ae",
   "metadata": {},
   "source": [
    "## Calculate similarity matrix, weirdness scores and T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e22e94-5369-4f12-a8ac-2e5714158708",
   "metadata": {},
   "outputs": [],
   "source": [
    "if retrain:\n",
    "    print('Applying the RF on the full dataset (real spectra only)')\n",
    "    X_train_leaves = rf.apply(X_train).astype(np.float32)\n",
    "\n",
    "    print('Predicting fully')\n",
    "    Y_train_hat = rf.predict_full_from_leaves(X_train_leaves).astype(np.float32)\n",
    "    \n",
    "    print('Saving the data')\n",
    "    to_s3_npy(X_train_leaves, s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'X_train_leaves.npy'))\n",
    "    to_s3_npy(Y_train_hat, s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'Y_train_hat.npy'))\n",
    "else:\n",
    "    print('loading the data')\n",
    "    X_train_leaves = from_s3_npy(s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'X_train_leaves.npy'))\n",
    "    Y_train_hat = from_s3_npy(s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'Y_train_hat.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550444f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if retrain:\n",
    "    print('Calculating the distance matrix')\n",
    "    from CustomRandomForest import build_distance_matrix\n",
    "    dist_mat = build_distance_matrix(X_train_leaves, Y_train_hat)\n",
    "\n",
    "    if save_RF_dis_mat:\n",
    "        print('Saving the distance matrix')\n",
    "        to_s3_npy(dist_mat, s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'dist_mat.npy'))\n",
    "else:\n",
    "    if save_RF_dis_mat:\n",
    "        print('Loading the distance matrix')\n",
    "        dist_mat = load_s3_npy(s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'dist_mat.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c107aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if retrain:\n",
    "    print('Calculating the weirdness scores')\n",
    "    weird_scores = np.mean(dist_mat, axis=1)\n",
    "\n",
    "    print('Saving the weirdness scores')\n",
    "    to_s3_npy(weird_scores, s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'weird_scores.npy'))\n",
    "else:\n",
    "    print('Loading the weirdness scores')\n",
    "    weird_scores = from_s3_npy(s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'weird_scores.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9f596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if retrain:\n",
    "    print('Running T-SNE')\n",
    "    from sklearn.manifold import TSNE\n",
    "    sne = TSNE(n_components=2, perplexity=25, metric='precomputed', verbose=1, random_state=seed).fit_transform(dist_mat)\n",
    "\n",
    "    print('Saving T-SNE')\n",
    "    to_s3_npy(sne, s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'tsne.npy'))\n",
    "else:\n",
    "    print('Loading T-SNE')\n",
    "    sne = to_s3_npy(s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'tsne.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db9ca51",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772ac628",
   "metadata": {},
   "outputs": [],
   "source": [
    "if retrain:\n",
    "    fig = plt.figure()\n",
    "    tmp = plt.hist(weird_scores, bins=60, color=\"g\")\n",
    "    plt.title(\"Weirdness score histogram\")\n",
    "    plt.ylabel(\"N\")\n",
    "    plt.xlabel(\"weirdness score\")\n",
    "\n",
    "    to_s3_fig(fig, s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'weirdness_scores_histogram.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c575e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "if retrain:\n",
    "\n",
    "    fig = plt.figure()\n",
    "    tmp = plt.hist(dist_mat[np.tril_indices(dist_mat.shape[0], -1)], bins=100)\n",
    "    plt.title(\"Distances histogram\")\n",
    "    plt.ylabel(\"N\")\n",
    "    plt.xlabel(\"distance\")\n",
    "\n",
    "    to_s3_fig(fig, s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'distances_histogram.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e13df62",
   "metadata": {},
   "outputs": [],
   "source": [
    "if retrain:\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,7))\n",
    "    ax = fig.add_subplot(111)\n",
    "    im_scat = ax.scatter(sne[:, 0], sne[:, 1], s=3, c=weird_scores, cmap=plt.cm.get_cmap('jet'), picker=1)\n",
    "    ax.set_xlabel('t-SNE Feature 1')\n",
    "    ax.set_ylabel('t-SNE Feature 2')\n",
    "    ax.set_title(r't-SNE Scatter Plot Colored by Weirdness score')\n",
    "    clb = fig.colorbar(im_scat, ax=ax)\n",
    "    clb.ax.set_ylabel('Weirdness', rotation=270)\n",
    "    plt.show()\n",
    "\n",
    "    to_s3_fig(fig, s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'tsne_colored_by_weirdness.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17e62ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111)\n",
    "import matplotlib.colors as colors\n",
    "snr = gs_train.loc[I_train].snMedian\n",
    "im_scat = ax.scatter(sne[:,0], sne[:,1], s=3, c=snr, cmap=plt.cm.get_cmap('jet'), norm=colors.LogNorm(vmin=snr.min(), vmax=80))\n",
    "ax.set_xlabel('t-SNE Feature 1')\n",
    "ax.set_ylabel('t-SNE Feature 2')\n",
    "ax.set_title(r't-SNE Scatter Plot Colored by SNR')\n",
    "clb = fig.colorbar(im_scat, ax=ax)\n",
    "clb.ax.set_ylabel('SNR', rotation=270)\n",
    "plt.show()\n",
    "\n",
    "to_s3_fig(fig, s3_client, bucket_name, os.path.join(s3_urf_save_dir_path, 'tsne_colored_by_snr.png'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
